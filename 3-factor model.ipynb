{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the group project for the MDS5001 Introduction to Python\n",
    "# Date: 2019.8.12\n",
    "# Build our own 3-Factor Asset Pricing Model\n",
    "\n",
    "# Stocks number must be over 30, plus a market index\n",
    "# Data source: Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary library\n",
    "\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# safely disable this new warning with the following assignment.\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the class to get more information\n",
    "# from https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression\n",
    "\n",
    "class LinearRegression(linear_model.LinearRegression):\n",
    "    \"\"\"\n",
    "    LinearRegression class after sklearn's, but calculate t-statistics\n",
    "    and p-values for model coefficients (betas).\n",
    "    Additional attributes available after .fit()\n",
    "    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n",
    "    which is (n_features, n_coefs)\n",
    "    This class sets the intercept to 0 by default, since usually we include it\n",
    "    in X.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LinearRegression, self)\\\n",
    "                .__init__(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X, y, n_jobs=1):\n",
    "        self = super(LinearRegression, self).fit(X, y, n_jobs)\n",
    "        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
    "        se = np.array([\n",
    "            np.sqrt(np.diagonal(sse[i] * np.linalg.inv(np.dot(X.T, X))))\n",
    "                                                    for i in range(sse.shape[0])\n",
    "                    ])\n",
    "\n",
    "        self.t = self.coef_ / se\n",
    "        self.p = 2 * (1 - stats.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1]))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scraping,  Data processing and Feature Engineering\n",
    "\n",
    "## Data scraping\n",
    "\n",
    "- data from yahoo finance & daily treasury\n",
    "- **stocks** use a list of sp500 firms, which is sorted by the market cap of firms\n",
    "- **market index** use sp500\n",
    "- **risk free rate** use 1-month treasury bill rate\n",
    "- use ```request.get()``` and ```pd.read_html()```\n",
    "\n",
    "### Notice that we can only scrap the data of 100 days at one time, so we need to scrap more than once\n",
    "\n",
    "- caculate epoch time of 100 days\n",
    "- scrap from the end date, and minus Epoch Time of 100 days in loop, until start date is bigger than end date\n",
    "\n",
    "### Save data\n",
    "\n",
    "- After scraping all the historical data of sp500, use ```to_csv()``` to save them\n",
    "- Then, we can load data from laptop instead of the Internet ( much faster )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: change format date into epoch time\n",
    "def dateToEpoch(date):\n",
    "    time_tuple = time.strptime(date, '%Y-%m-%d')\n",
    "    time_epoch = time.mktime(time_tuple)\n",
    "    time_tostr = str(int(time_epoch))\n",
    "    \n",
    "    return time_tostr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: used to scrap data from yahoo finance\n",
    "# parameter: \n",
    "#     data_list  - a list of string, stocks name or market index\n",
    "#     start_date - historical data start date\n",
    "#     end_date   - historical data end date\n",
    "# return:\n",
    "#     df_list    - a list of dataframe\n",
    "\n",
    "def dataScraping(data_list, start_date, end_date):\n",
    "    \n",
    "    yahoo_finance_url = 'https://au.finance.yahoo.com/quote/%s/history?period1=%s&period2=%s&interval=1d&filter=history&frequency=1d'\n",
    "    df_list = []\n",
    "    count = 0\n",
    "    one_hundred_epoch = 60 * 60 * 24 * 100      # used to get 100 days of data step by step\n",
    "    \n",
    "    for data in data_list:\n",
    "        print(\"Scraping\", data, \"...\")\n",
    "        # try to get web page if end_date > start_date\n",
    "        first_round = True\n",
    "        temp_date   = end_date\n",
    "        while temp_date > start_date:\n",
    "            yahoo_hist_price_page = rq.get(yahoo_finance_url % (data, start_date, temp_date))\n",
    "            if yahoo_hist_price_page.status_code == 200:\n",
    "                if first_round:\n",
    "                    df_list.append(pd.read_html(yahoo_hist_price_page.text)[0].iloc[:-1]) # exclude last row\n",
    "                    first_round = False\n",
    "                else:\n",
    "                    df_combined = pd.read_html(yahoo_hist_price_page.text)[0].iloc[:-1]\n",
    "                    df_list[count] = pd.concat([df_list[count], df_combined], axis=0)     # more efficient\n",
    "                    # df_list[count] = df_list[count].append(pd.read_html(yahoo_hist_price_page.text)[0].iloc[:-1])\n",
    "            else:\n",
    "                raise Exception('Some Error Happened. Http Error Code %d' % yahoo_hist_price_page.status_code)\n",
    "\n",
    "            temp_date = str(int(temp_date) - one_hundred_epoch)\n",
    "\n",
    "        count += 1\n",
    "    print(\"----------------------\\nScraping process done!\")\n",
    "    \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sp500 firms name, which is sorted by market cap\n",
    "result=[]\n",
    "with open('SP500_Stocks.txt','r') as f:\n",
    "    for line in f:\n",
    "        result.append(line.strip('\\n').split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data\n",
    "\n",
    "security_list   = result                      # Security list\n",
    "market_index_list = ['%5EGSPC']               # Market index list\n",
    "start_date = dateToEpoch('2018-07-01')\n",
    "end_date   = dateToEpoch('2019-06-30')\n",
    "\n",
    "# data from yahoo finance, stocks and market index\n",
    "# stocks_df = dataScraping(security_list, start_date, end_date)\n",
    "# market_index_df = dataScraping(market_index_list, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from daily treasury\n",
    "# scraping the 1-month treasury bill rate 2018 & 2019\n",
    "\n",
    "years = ['2018', '2019']\n",
    "treasury_url = 'https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=billRatesYear&year=%s'\n",
    "first_bool = True\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    treasury_page = rq.get(treasury_url % year)\n",
    "\n",
    "    if treasury_page.status_code == 200:\n",
    "        bill_rate_df = pd.read_html(treasury_page.text)[1]\n",
    "    else:\n",
    "        raise Exception('Some Error Happened. Http Error Code %d' % treasury_page.status_code)\n",
    "    \n",
    "    if first_bool:\n",
    "        bill_rate_month_df = pd.DataFrame()\n",
    "        bill_rate_month_df['Date'] = bill_rate_df['Unnamed: 0_level_0']['DATE']\n",
    "        bill_rate_month_df['Month Rate'] = bill_rate_df['4 WEEKS']['BANK DISCOUNT']\n",
    "        # convert time to datetime\n",
    "        bill_rate_month_df['Date'] = pd.to_datetime(bill_rate_month_df['Date'])\n",
    "    else:\n",
    "        new_month_df = pd.DataFrame()\n",
    "        new_month_df['Date'] = bill_rate_df['Unnamed: 0_level_0']['DATE']\n",
    "        new_month_df['Month Rate'] = bill_rate_df['4 WEEKS']['BANK DISCOUNT']\n",
    "        # convert time to datetime\n",
    "        new_month_df['Date'] = pd.to_datetime(new_month_df['Date'])\n",
    "        \n",
    "        # concat two dataframe\n",
    "        bill_rate_month_df = pd.concat([bill_rate_month_df, new_month_df], axis = 0)\n",
    "\n",
    "    first_bool = False\n",
    "\n",
    "bill_rate_month_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data to csv\n",
    "\n",
    "# bill_rate_month_df.to_csv('bill_rate.csv', index=False)\n",
    "# market_index_df[0].to_csv('market_index.csv', index=False)\n",
    "# for i in range(len(stocks_df)):\n",
    "#    stocks_df[i].to_csv('./data/stocks' + str(i) + '.csv')\n",
    "# for i in range(len(market_index_df)):\n",
    "#     market_index_df[i].to_csv('./data/market' + str(i) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "- load data from csv\n",
    "\n",
    "### Data cleaning\n",
    "\n",
    "- drop duplicate rows, drop rows contain UNEXPECTED STRING\n",
    "- change the value to numerical value\n",
    "- change date format used ```pd.to_datetime()```\n",
    "\n",
    "### Data pre-processing\n",
    "\n",
    "- add stocks name as a column\n",
    "- merge bill rate dataframe\n",
    "- reverse the dataframe\n",
    "- reset the index of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data from csv saved before\n",
    "\n",
    "# Load stocks data from 2018/7/1 to 2019/6/30\n",
    "stocks_df = []\n",
    "for i in range(len(security_list)):\n",
    "    stocks_df.append(pd.read_csv('./data/stocks' + str(i) + '.csv').drop(['Unnamed: 0'],axis=1))\n",
    "\n",
    "# Load market index\n",
    "market_index_df = []\n",
    "for i in range(len(market_index_list)):\n",
    "    market_index_df.append(pd.read_csv('market_index.csv'))\n",
    "# Load bill rate\n",
    "bill_rate_month_df = pd.read_csv('bill_rate.csv')\n",
    "bill_rate_month_df['Date'] = pd.to_datetime(bill_rate_month_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: used to cleaning data\n",
    "\n",
    "def dataCleaning(df_list, data_list):\n",
    "    for i in range(len(df_list)):\n",
    "        # Drop 'dividend' rows\n",
    "        if np.issubdtype(df_list[i]['Open'][-1:], np.string_):\n",
    "            df_list[i] = df_list[i][~df_list[i]['Open'].str.contains('Dividend')]\n",
    "        \n",
    "        # change numeric str to numeric values\n",
    "        for column in df_list[i].columns:\n",
    "            if column == 'Date':\n",
    "                continue\n",
    "            df_list[i][column] = pd.to_numeric(df_list[i][column], errors='coerce')\n",
    "        # Drop duplicated data\n",
    "        df_list[i].drop_duplicates([\"Date\"], keep=\"first\", inplace=True)\n",
    "\n",
    "        # Add security name as a column\n",
    "        df_list[i]['Name'] = str(data_list[i])\n",
    "        # Convert time format\n",
    "        df_list[i]['Date'] = pd.to_datetime(df_list[i]['Date'])\n",
    "        # Merge risk-free rate by Date\n",
    "        df_list[i] = pd.merge(df_list[i], bill_rate_month_df, how='inner', on=['Date'])\n",
    "        # Reverse dataframe\n",
    "        df_list[i] = df_list[i][::-1]\n",
    "        # Reset index\n",
    "        df_list[i] = df_list[i].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Data\n",
    "dataCleaning(stocks_df, security_list)\n",
    "dataCleaning(market_index_df, market_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is NAN value\n",
    "# print(stocks_df[0].isnull().any()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "for i in range(len(market_index_df)):\n",
    "    market_index_df[i]['Pct_change'] = market_index_df[i]['Adj. close**'].pct_change(periods = 1)\n",
    "    market_index_df[i]['Rm - Rf'] = market_index_df[i]['Pct_change'] - stocks_df[0]['Month Rate'].shift() / 100 / 365\n",
    "\n",
    "for i in range(len(stocks_df)):\n",
    "    \n",
    "    # Ri of stocks\n",
    "    stocks_df[i]['Pct_change'] = stocks_df[i]['Adj. close**'].pct_change(periods = 1)\n",
    "    # Ri - Rf of stocks\n",
    "    stocks_df[i]['Ri - Rf'] = stocks_df[i]['Pct_change'] - stocks_df[0]['Month Rate'].shift() / 100 / 365\n",
    "    # Log volume\n",
    "    stocks_df[i]['Log_Volume'] = np.log(stocks_df[i]['Volume'])\n",
    "    \n",
    "    # Rmkt - Rf from market_index, used sp500\n",
    "    stocks_df[i] = pd.merge(stocks_df[i], market_index_df[0][['Date', 'Rm - Rf']], how='inner', on=['Date'])\n",
    "\n",
    "    \n",
    "#   Caculate log return for each stock\n",
    "#   stocks_df[i]['Log_return'] = np.log(stocks_df[i]['Adj. close**']) / np.log(stocks_df[i]['Adj. close**'].shift(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featuring factor of Rs - Rm\n",
    "\n",
    "mid = len(stocks_df) // 2\n",
    "\n",
    "big_merge_df   = stocks_df[0][['Date', 'Pct_change']]\n",
    "small_merge_df = stocks_df[mid][['Date', 'Pct_change']]\n",
    "for i in range(1, mid):\n",
    "    big_merge_df = pd.merge(big_merge_df, stocks_df[i][['Date', 'Pct_change']], how='outer', on=['Date'])\n",
    "for i in range(mid+1, len(stocks_df)):\n",
    "    small_merge_df = pd.merge(small_merge_df, stocks_df[i][['Date', 'Pct_change']], how='outer', on=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate the mean of big and small company returns\n",
    "big_merge_df['mean return']   = big_merge_df.mean(axis=1)\n",
    "small_merge_df['mean return'] = small_merge_df.mean(axis=1)\n",
    "\n",
    "new_merge_df = pd.merge(big_merge_df[['Date', 'mean return']], small_merge_df[['Date', 'mean return']], \\\n",
    "                        how='inner', on=['Date'])\n",
    "new_merge_df['Rs - Rb'] = new_merge_df['mean return_y'] - new_merge_df['mean return_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge Rs - Rb in stocks_df\n",
    "for i in range(len(stocks_df)):\n",
    "    stocks_df[i] = pd.merge(stocks_df[i], new_merge_df[['Date', 'Rs - Rb']], how='inner', on=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the regression and save information\n",
    "score_list = []\n",
    "name_list  = []\n",
    "alpha_list = []\n",
    "F1_list = []\n",
    "F2_list = []\n",
    "F3_list = []\n",
    "P_F1_list = []\n",
    "P_F2_list = []\n",
    "P_F3_list = []\n",
    "\n",
    "factor_list = ['Rm - Rf', 'Rs - Rb', 'Log_Volume']\n",
    "\n",
    "for stock in stocks_df:\n",
    "    \n",
    "    # drop rows contain nan\n",
    "    stock.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    if stock.shape[0] <= 0:\n",
    "        continue\n",
    "    \n",
    "    X_train = stock[factor_list]\n",
    "    Y_train = stock['Ri - Rf']\n",
    "    \n",
    "    # data standardised\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    for factor in factor_list:\n",
    "        X_train[factor] = scaler.fit_transform(X_train[factor].values.reshape(-1,1))\n",
    "    Y_train = scaler.fit_transform(Y_train.values.reshape(-1,1))\n",
    "    \n",
    "    # fit linear regression model \n",
    "    reg = LinearRegression(normalize=True, n_jobs=4).fit(X_train, Y_train)\n",
    "    \n",
    "    # Save information\n",
    "    score_list.append(reg.score(X_train, Y_train))\n",
    "    name_list.append(stock['Name'][-1:].values[0])\n",
    "    alpha_list.append(reg.intercept_[0])\n",
    "    F1_list.append(reg.coef_[0, 0])\n",
    "    F2_list.append(reg.coef_[0, 1])\n",
    "    F3_list.append(reg.coef_[0, 2])\n",
    "    P_F1_list.append(reg.p[0, 0])\n",
    "    P_F2_list.append(reg.p[0, 1])\n",
    "    P_F3_list.append(reg.p[0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train['Rm - Rf'], Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train['Rs - Rb'], Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train['Log_Volume'], Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an output dataframe for all the regression models\n",
    "output_df = pd.DataFrame(columns=['Name', 'R^2', 'alpha', 'F1', 'F2', 'F3', 'P_F1', 'P_F2', 'P_F3'])\n",
    "\n",
    "output_df['Name']  = name_list\n",
    "output_df['R^2']   = score_list\n",
    "output_df['alpha'] = alpha_list\n",
    "output_df['F1']    = F1_list\n",
    "output_df['F2']    = F2_list\n",
    "output_df['F3']    = F3_list\n",
    "output_df['P_F1']  = P_F1_list\n",
    "output_df['P_F2']  = P_F2_list\n",
    "output_df['P_F3']  = P_F3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis in different industry\n",
    "energy_list = ['XOM','CVX','COP','KMI','SLB','PSX','EOG','OXY','VLO','MPC','OKE','WMB','BHGE','PXD','HES']\n",
    "tech_list   = ['MSFT','AAPL','GOOG','GOOGL','FB','INTC','CSCO','ORCL','ADBE','CRM','ACN','IBM','TXN','AVGO','NVDA']\n",
    "health_list = ['JNJ','UNH','MRK','PFE','ABT','MDT','AMGN','TMO','LLY','DHR','ABBV','SYK','GILD','CVS','BMY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df = output_df[output_df['Name'].isin(energy_list)]\n",
    "tech_df   = output_df[output_df['Name'].isin(tech_list)]\n",
    "health_df = output_df[output_df['Name'].isin(health_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_stock_df = []\n",
    "for stock in stocks_df:\n",
    "    if (stock['Name'].isin(energy_list)).any():\n",
    "        energy_stock_df.append(stock)\n",
    "tech_stock_df = []\n",
    "for stock in stocks_df:\n",
    "    if (stock['Name'].isin(tech_list)).any():\n",
    "        tech_stock_df.append(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily return change in energy industry\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "plt.plot(energy_stock_df[0]['Date'], energy_stock_df[0]['Pct_change'], c='red')\n",
    "plt.plot(energy_stock_df[1]['Date'], energy_stock_df[1]['Pct_change'], c='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_merge_df = pd.DataFrame()\n",
    "for stock in energy_stock_df:\n",
    "    energy_merge_df = pd.concat([energy_merge_df, stock], axis = 0)\n",
    "tech_merge_df = pd.DataFrame()\n",
    "for stock in tech_stock_df:\n",
    "    tech_merge_df = pd.concat([tech_merge_df, stock], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average return in energy & technology industry\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "grouped_energy_mean = energy_merge_df.groupby('Date')['Pct_change'].mean()\n",
    "grouped_tech_mean = tech_merge_df.groupby('Date')['Pct_change'].mean()\n",
    "plt.plot(grouped_energy_mean.index, grouped_energy_mean, c='purple', label='energy')\n",
    "plt.plot(grouped_tech_mean.index, grouped_tech_mean, c='blue', label='tech')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
